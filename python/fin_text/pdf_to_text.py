# -*- coding: utf-8 -*-
"""pdf_to_text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11TVofTAR0nhTBtQejUe5AoZE-LsN6ptY
"""

from google.colab import drive
drive.mount('/content/drive')

!apt install poppler-utils

!pip install zenhan

import subprocess
import zenhan
import os
import sys
import re
import unicodedata
import argparse

def read_args():
  parser = argparse.ArgumentParser(description="A sentence extraction program from PDF")
  parser.add_argument('file', type=str, help="Target PDF file")
  return parser.parse_args()

def normalize(sentence):
  sentence = unicodedata.normalize('NFKC', sentence)
  sentence = zenhan.h2z(sentence)
  sentence = sentence.replace(u' ', u'').replace(u'　', u'')
  return re.sub("\s*", u'', sentence)

def mk_press_data(pdf):
  mat_eol = re.compile('\n'.encode('utf-8'))
  sentences = []
  p = subprocess.run(('pdftotext', pdf, '-'), stdout = subprocess.PIPE,
                     stderr = subprocess.PIPE)
  temp = u""
  for line in mat_eol.split(p.stdout):
    try:
      line = line.decode('utf-8')
    except:
      line = ""
    if line == "":
      if temp != "" and u"。" in temp:
        sentences = sentences + re.split(u"。", temp)
      temp = u""
    else:
      temp = temp + line
  if temp != "" and u"。" in temp:
    sentences = sentences + re.split(u"。", temp)
  
  # 分を整形、文末に句点を追加、2文字以下もしくは201文字以上を削除
  sentences = [normalize(line + u"。")
      for line in sentences if len(line) <= 200 and len(line) >=3]
  return sentences

sentences = mk_press_data('/content/drive/MyDrive/Colab Notebooks/fin_text/toyota_2021_03.pdf')
for sentence in sentences:
  print(sentence)